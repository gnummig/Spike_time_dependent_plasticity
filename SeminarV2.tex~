\documentclass{SeminarV2}
\usepackage[dvips]{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{amssymb,amsmath,array}
\usepackage{natbib}

%***********************************************************************
% !!!! IMPORTANT NOTICE ON TEXT MARGINS !!!!!
%***********************************************************************
%
% Please avoid using DVI2PDF or PS2PDF converters: some undesired
% shifting/scaling may occur when using these programs
% It is strongly recommended to use the DVIPS converters.
%
% Check that you have set the paper size to A4 (and NOT to letter) in your
% dvi2ps converter, in Adobe Acrobat if you use it, and in any printer driver
% that you could use.  You also have to disable the 'scale to fit paper' option
% of your printer driver.
%
% In any case, please check carefully that the final size of the top and
% bottom margins is 5.2 cm and of the left and right margins is 4.4 cm.
% It is your responsibility to verify this important requirement.  If these margin requirements and not fulfilled at the end of your file generation process, please use the following commands to correct them.  Otherwise, please do not modify these commands.
%
\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}


\begin{document}
%style file for Seminar manuscripts
\title{Spike-timing Based Image Processing: Can we Reproduce Biological Vision in Hardware}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
\author{Daniel Mayer 
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
% Addresses and institutions
University Leipzig - Master of Science in Bioinformatics}
%
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

\begin{abstract}
In here I present a review of research and technologies using asynchronous spike-based processing strategies as can be observed in biological vision.
The research presented includes a general description of the issue \cite{thorpe_spike-based_2012}, the presentation of a 
silicon retina \cite{lichtsteiner_128_2008} as input device as well as the current research on memristive devices \cite{jo_high-density_2009}.
while the common approach to emulate neural networks is simulation on conventional hardware as CPU's and GPU's this review is rather focussed on the efforts of neuromorphics to reproduce the property of neural structures to work vastly parallel. In special this review will look at the why the biological vision system is able to compute relevant information with just one spike per neuron, due to the relative timing of spikes and the consequences on neuromorphic systems.
\end{abstract}

\section{Biological Vision and its spike-timing-based component} 
A lot of research considers spike frequency to be the parameter in which stimuli are encoded in the nervous system. It is obvious to come to such a conclusion, since it is comparatively easy to measure the spiking frequency of a single neuron in dependence to a stimulus.
But the speed of biological vision poses a problem on conventional views of how information is processed in the brain.
As will be demonstrated, the notion of spike-frequency encoding of information contradicts speed at which neurons from the highest level of the primate vision system can respond selectively to complex stimuli such as faces.
Studies have shown that such processes can be accomplished in as much as 100ms, which combined with the fact, that cortical neurons seldomly fire faster than every 10 ms and that there are about 10 stages of neurons involved between the photo-receptors and the high level neurons, leads to the conclusion that each neuron has in average only time to fire once.
It is therefore proposed, that rather than the information being only encoded in firing frequencies corresponding to analog values,
the relative timing of the spikes is also used to encode such information \cite{thorpe_biological_1989} \cite{bialek_reading_1991} \cite{gerstner_why_1993}. It is though harder to measure, since one has to use multiple electrodes to measure relative spike timings between neurons.
Still  the idea has been demonstrated experimentally using recordings from the salamander retina \cite{gollisch_rapid_2008}.
Simulations also show, that it is possible to do state of the art face recognition using an approach 
where every neuron is only allowed to fire at most once \cite{thorpe_spike-based_2012}.
It is inherent that if stimuli are encoded in the spike timing, also the neural plasticity needs to depend on relative spike timing, which leads to the notion of spike-time-dependent-plasticity (STDP)

\section{Silicon retina}
Conventional digital imaging chips have the immanent property of a frame-rate, which can be interpreted as a relict from the analog imaging times. All incoming information is collected in the sensor until the next frame is read out. For every frame the imaging chip sends out a pixel map containing the corresponding absolute value of each pixel. This yields massive output data regardless of the actual information((e.g. $>$1 GB/s from 352x288 pixels at 10 kFPS in \cite{kleinfelder_10000_2001})),   Biological vision on the other hand is not bound to frames but is an event-based (asynchronous) relative intensity vision, meaning that the change of an individual pixel is send as soon as it registered. There are two essential advantages of the latter approach, which are (i) the speed of reception of individual events, and (ii) a natural compression, removing redundant information.\\
With regard to these differences, I refer a silicon retina as proposed in \cite{lichtsteiner_128_2008} which is a 128x128 pixel Cmos sensor, that outputs data in Adress-event representation (AER). The device improves on prior frame based temporal
difference detection imagers (e.g., \cite{mallik_temporal_2005}) by asynchronously responding to temporal contrast rather than absolute illumination.

\subsection{Adress-event representation (AER)}
The modular design of computer hardware leads to bandwidth bottlenecks between the individual components. Thus unlike in the nervous system, where data transmission happens through many neurons in parallel, data needs to be multiplexed onto a bus, connecting the modules. The basic idea behind AER is to encode the visual stimuli into address events that are locally generated by the pixels. In this particular silicon retina the address event consists of 2x7 bits for the $x$ and $y$ coordinates and one ON/OFF polarity bit \cite{lichtsteiner_128_2008}.\\
There has been some other research about AER imaging devices, but apart from that, the field of AER devices is largely unexplored.
One reason could be that computer vision is only becoming popular now that computer power is sufficient to deal with the incoming information, so there has been little need for asynchronous data.

\section{Memristive Neural Networks}
In current Computer hardware information storage and computation are done in separate units, whereas in nature a neuron acts as a computational unit that keeps information stored in the synapses to other neurons. When modeling neural behaviour a spike signal has to be simulated on hardware that is foreign to the notion of spikes and relative timing, but abides in the realm of clocked binary logic. In fact an emulation of a part of the cat brain has been done on the BlueGene/P using 147 456 CPUs and 144 TB of main memory and a power consumption in the MW regime, but still being 83 times slower than the cat \cite{jo_nanoscale_2010}. This ineficiency among other things due to the so called Neuman bottleneck\cite{jo_nanoscale_2010}. To overcome the these constraints, the recent discovery of the yet foreseen \cite{chua_memristor-missing_1971} fourth two-terminal canonical circuit element named memristor has lead computational neuroscientsts to dream of implementing a neural network directly on chip.

\subsection{Memristor}
Proposed theoretically in 1971 by Chua \cite{chua_memristor-missing_1971} and eventually identified in 2008 by Strukov et al, the memristor is a electrical component that relates charge $q$ to magnetic flux $\varphi$. It is supposed, that it took nearly 40 years to experimentally prove Chuas prediction, because people were simply searching in the wrong place, mislead by the erroneous assumption that it had to be something dealing with magnetism, infered from that it related the magnetic flux.
The type of memristor found by Strukov had in fact nothing to do with magnetism, but the effect is created by dopant ions drifting in a thin layer of titanium dioxide \cite{strukov_missing_2008}.
What makes the memristor exceptionally interesting, is that it retains the property of 'memorizing' the charge that has flown through. this happens by moving the dopant ions through the titanium dioxide according to the voltage across it, thus changing its resistance by changing the size of the volume that is made more conductive through the dopant. The memristor stores its history in its resistance, giving it the name memristor as a contraction of the words 'memory' and 'resistor'.
The effect is reversed when reversing the voltage across the memristor leaving it unchanged by a symmetric alternating signal giving the possibility to read the value of the memristor without changing it.
Because of the analog nature it is at the moment possible to reliably store 3 bit in a single memristor \cite{nima_taherinejad_memristors_????}. Apart from that memristor arrays can be manufactured much denser than  NAND flash or magnetic hard drives, leading to an even higher storage density than what is physically possible with present-day storage techniques.

\subsection{Memristor Arrays for Neuromorphic Systems}
The property of keeping track of its past is very similar to the plasticity of synapses. Hence it has been proposed to use memristor arrays as synapses for neuromorphic systems. The proposed structure consists of crossbar arrays of memristors that connect artificial spiking neurons made up of a few transistors \cite{linares-barranco_memristance_2009}. A network of this form wouldn't be trained by back propagation or other rather sophisticated techniques, but would be self learning Thus it is  even nearly impossible to supervise or debug the process at a neural level not to mention  extracting the learned information and putting it into a repository, usable by other computers.
Since the memristor value is changed upon a bias voltage along the two terminals it is obvious, that plasticity can only come from relative spike timing and non-linearity of the memristic behaviour. Two neurons spiking synchronously at high frequency would leave the memristor value unchanged, as the potential difference across the memristor would stay very small. Spikes would have to be symmetric, for reasons pointed out above and the only chance of a memristor reacting to two well timed spikes is, that its change of resistance is nonlinear to the bias voltage \cite{zdenek_biolek_spice_????}\cite{nima_taherinejad_memristors_????} $dR = V^\alpha dt$ with $\alpha > 1$.
%Beig a fairly recent discovery there are still some hurdles to overcome though
% high power consumption here
%But is evident, that when emulating brain functionality
\section{Discussion}
Spike based hardware is still under heavy development but the research done so far shows promising results for computer vision and neural networks. Most of the devices built today are purely academic. 
But it is unavoidable to research on STDP if we truly want to understand the dynamics of biological neural networks, and it could well lead to ways of building sensory implants that interact in a more natural way with the central nervous system.\\
Memristors being a fairly recent discovery there are still some problems to overcome. Even though the potential of memristive devices in neuromorphics is evident, it is yet to be seen if memristors will be available with resistances high enough and operating voltages low enough to keep power consumption low enough. Assuming wafer scale dies of 100 nm pitch which could be reached soon, it is well possible to put $10^6$ neurons with $10^10$ synapses on a $cm^2$, but at 1 V and 1$M\Omega$ a spike frequency of 10 Hz with a spike duration of about 20ms that would lead to a power dissipation of 2 kW by the memristors alone.\\
In the biological case transmission of stimuli takes place at a mere 1-2 m/s, which combined with the length of axons yields a time base for relative and frequency timing. On a neuromorphic system data transmission happens nearly at speed of light with connections being even shorter than the axons so there is no relevant immanent latency and hence the timing of signals is going to have little context.\\
A particularly interesting fact is, as mentioned above, that neuromorphic systems are going to be very different to machines that we know today. Because programming will be similar to biological learning, unless trained in exactly the same way, the knowledge of one device will be physically bound to that device impeding possible 'gleichschaltung' in the sense that we attribute to software being installed on computers.\\
Spike timing dependent cameras with a small neuronal network attached to it might soon be used let's say for self driving cars, and they will not be objective any more, but will actively filter data according to their own(very limited) knowledge before transmitting it to the CPU or anything else.\\
To answer the question of the title: Steps have beend taken to make computer vision more nature like and they have been effectively used in particular applications. more nature like computer vision will have to be used in future, when conventional hardware reaches its limit considering speed or time resolution producing an inconsumable data stream. But for now there are no commercial solutions as the topic is mainly still under development.
\iffalse
there are 10e11 neurons in the human brain, each having up to 15.000 synapses thus 10e15 synapses which is equivalent to petabyte
\fi

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\begin{footnotesize}


\bibliographystyle{unsrt}
\bibliography{SeminarV2.bib}

\end{footnotesize}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}
